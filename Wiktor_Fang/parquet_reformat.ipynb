{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_row_id(row):\n",
    "    return str(row.frame) + \"-\" + row.type + \"-\" + str(row.landmark_index)\n",
    "\n",
    "def reset_frame_num(pf):\n",
    "    frames = pf.frame.unique()\n",
    "    frames_cnt = len(frames)\n",
    "\n",
    "    lh_cnt = len(pf.loc[(pf.frame==min(frames))&(pf.type==\"left_hand\")])\n",
    "    rh_cnt = len(pf.loc[(pf.frame==min(frames))&(pf.type==\"right_hand\")])\n",
    "    ps_cnt = len(pf.loc[(pf.frame==min(frames))&(pf.type==\"pose\")])\n",
    "\n",
    "    points_p_frame = lh_cnt + rh_cnt + ps_cnt\n",
    "\n",
    "    new_frame_nums = [[i for _ in range(points_p_frame)] for i in range(frames_cnt)]\n",
    "    new_frame_nums = sum(new_frame_nums, [])\n",
    "\n",
    "    pf.frame = new_frame_nums\n",
    "    pf.row_id = pf.apply(get_row_id, axis=1)\n",
    "    return pf\n",
    "\n",
    "def convert_parquet_to_np_format(pf):\n",
    "    arrayd_parquet = []\n",
    "    types = [\"pose\",\"left_hand\",\"right_hand\"]\n",
    "\n",
    "    for frame in pf.frame.unique():\n",
    "        frame_arr = np.array([])\n",
    "        frame_chunk = pf.loc[pf.frame==frame].sort_values('landmark_index')\n",
    "        \n",
    "        for type in types:\n",
    "            xyz_vals = frame_chunk.loc[frame_chunk.type==type, [\"x\",\"y\",\"z\"]].values\n",
    "            xyz_vals = np.concatenate(xyz_vals)\n",
    "\n",
    "            frame_arr = np.concatenate([frame_arr, xyz_vals])\n",
    "\n",
    "        arrayd_parquet.append(frame_arr)\n",
    "\n",
    "    return arrayd_parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from os import listdir\n",
    "# from os.path import isfile, join\n",
    "# import sys\n",
    "\n",
    "\n",
    "# folders = [\"alligator/\", \"flower/\", \"kiss/\", \"listen/\", \"orange/\"]\n",
    "# types_pad = [[0.0 for i in range(33*3)],[0.0 for i in range(33*3)],[0.0 for i in range(33*3)]]\n",
    "\n",
    "# for folder in folders:\n",
    "#     path = \"./asl-kaggle/by_labels/\"+folder\n",
    "#     save_path = \"./asl-kaggle/no_preprocess_np_labels/\"+folder\n",
    "#     parquets = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "\n",
    "#     for parquet in parquets[:50]:\n",
    "#         parquet_path = path+parquet\n",
    "#         pf = pd.read_parquet(parquet_path)\n",
    "        \n",
    "#         pf = pf.drop(pf.loc[pf.type==\"face\"].index)\n",
    "#         pf = pf.fillna(0)\n",
    "#         pf = pf.sort_values('frame')\n",
    "\n",
    "#         pf['xyz'] = pf.apply(bond_xyz, axis=1)\n",
    "#         np_array = convert_parquet_to_np_format(pf)\n",
    "\n",
    "#         np_array = np_array + [types_pad for i in range(267 - len(np_array))]\n",
    "#         np_array = np.array(np_array)\n",
    "        \n",
    "#         np.save(save_path+parquet, np_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicate_vals(pf, values: np.array, iter):\n",
    "    frames_to_dup = pf.loc[pf.frame.isin(values)].copy()\n",
    "    frames_to_dup.frame += (frames_to_dup.frame-round(frames_to_dup.frame))*0.001 + 0.01*iter\n",
    "    \n",
    "    frames_to_dup.row_id = frames_to_dup.apply(get_row_id, axis=1)\n",
    "\n",
    "    pf = pd.concat([pf, frames_to_dup], ignore_index=True).sort_values('frame')\n",
    "\n",
    "    return pf\n",
    "\n",
    "# def averaged_duplicate(pf, values: np.array, iter):\n",
    "#     frames_to_dup = pf.loc[pf.frame.isin(values)].copy()\n",
    "#     ahead_1 = pf.loc[pf.frame.isin(values+1)].copy()\n",
    "\n",
    "#     if len(ahead_1) != len(frames_to_dup):\n",
    "#         ahead_1 = pd.concat([\n",
    "#             ahead_1,\n",
    "#             frames_to_dup.loc[frames_to_dup.frame==max(frames_to_dup.frame)]\n",
    "#             ], ignore_index=True)\n",
    "\n",
    "#     frames_to_dup.x = (ahead_1.x.values+frames_to_dup.x)/2\n",
    "#     frames_to_dup.y = (ahead_1.y.values+frames_to_dup.y)/2\n",
    "#     frames_to_dup.z = (ahead_1.z.values+frames_to_dup.z)/2\n",
    "\n",
    "#     frames_to_dup.frame += 0.01*iter\n",
    "\n",
    "#     pf = pd.concat([pf, frames_to_dup], ignore_index=True).sort_values('frame')\n",
    "\n",
    "#     return pf\n",
    "\n",
    "def remove_vals(pf, values: np.array):\n",
    "    values = np.sort(values)\n",
    "\n",
    "    for val in values:\n",
    "        pf = pf.loc[pf.frame != val]\n",
    "\n",
    "    return pf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(pf, frame_amt_goal, iter=1):\n",
    "\n",
    "    frame_nums = pf.frame.unique()\n",
    "    frame_diff = abs(frame_amt_goal - len(frame_nums))\n",
    "    operation = frame_amt_goal > len(frame_nums)\n",
    "\n",
    "    values_to_operate = np.array([])\n",
    "\n",
    "    if frame_diff%2 == 1:\n",
    "        central_point = frame_nums[int(len(frame_nums)/2)]\n",
    "        values_to_operate = np.append(values_to_operate, [central_point])\n",
    "        frame_nums = np.delete(frame_nums, [int(len(frame_nums)/2)])\n",
    "        frame_diff -= 1\n",
    "\n",
    "    if frame_diff != 0:\n",
    "        step_val = len(frame_nums)/frame_diff\n",
    "        step_val = 1 if step_val < 1 else step_val\n",
    "        \n",
    "        loop_cnt = len(frame_nums) if frame_diff > len(frame_nums) else frame_diff\n",
    "        values_to_operate = np.append(values_to_operate, frame_nums[[int(i*step_val) for i in range(0, loop_cnt)]])\n",
    "    else:\n",
    "        loop_cnt = 0\n",
    "\n",
    "    if operation:\n",
    "        pf = duplicate_vals(pf, values_to_operate, iter)\n",
    "    else:\n",
    "        pf = remove_vals(pf, values_to_operate)\n",
    "\n",
    "    if frame_diff - loop_cnt != 0:\n",
    "        pf = transform_data(pf, frame_amt_goal, iter+1)\n",
    "\n",
    "    return pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame</th>\n",
       "      <th>row_id</th>\n",
       "      <th>type</th>\n",
       "      <th>landmark_index</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22</td>\n",
       "      <td>22-face-0</td>\n",
       "      <td>face</td>\n",
       "      <td>0</td>\n",
       "      <td>0.452728</td>\n",
       "      <td>0.497625</td>\n",
       "      <td>-0.026592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>22-face-1</td>\n",
       "      <td>face</td>\n",
       "      <td>1</td>\n",
       "      <td>0.444277</td>\n",
       "      <td>0.469700</td>\n",
       "      <td>-0.053501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>22-face-2</td>\n",
       "      <td>face</td>\n",
       "      <td>2</td>\n",
       "      <td>0.446708</td>\n",
       "      <td>0.476862</td>\n",
       "      <td>-0.027491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22</td>\n",
       "      <td>22-face-3</td>\n",
       "      <td>face</td>\n",
       "      <td>3</td>\n",
       "      <td>0.438305</td>\n",
       "      <td>0.441203</td>\n",
       "      <td>-0.039765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22</td>\n",
       "      <td>22-face-4</td>\n",
       "      <td>face</td>\n",
       "      <td>4</td>\n",
       "      <td>0.444290</td>\n",
       "      <td>0.461555</td>\n",
       "      <td>-0.057071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5968</th>\n",
       "      <td>32</td>\n",
       "      <td>32-right_hand-16</td>\n",
       "      <td>right_hand</td>\n",
       "      <td>16</td>\n",
       "      <td>0.259397</td>\n",
       "      <td>0.725355</td>\n",
       "      <td>-0.272741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5969</th>\n",
       "      <td>32</td>\n",
       "      <td>32-right_hand-17</td>\n",
       "      <td>right_hand</td>\n",
       "      <td>17</td>\n",
       "      <td>0.159414</td>\n",
       "      <td>0.684204</td>\n",
       "      <td>-0.181114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5970</th>\n",
       "      <td>32</td>\n",
       "      <td>32-right_hand-18</td>\n",
       "      <td>right_hand</td>\n",
       "      <td>18</td>\n",
       "      <td>0.175069</td>\n",
       "      <td>0.677991</td>\n",
       "      <td>-0.243891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5971</th>\n",
       "      <td>32</td>\n",
       "      <td>32-right_hand-19</td>\n",
       "      <td>right_hand</td>\n",
       "      <td>19</td>\n",
       "      <td>0.188972</td>\n",
       "      <td>0.697104</td>\n",
       "      <td>-0.263154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5972</th>\n",
       "      <td>32</td>\n",
       "      <td>32-right_hand-20</td>\n",
       "      <td>right_hand</td>\n",
       "      <td>20</td>\n",
       "      <td>0.200690</td>\n",
       "      <td>0.713311</td>\n",
       "      <td>-0.274248</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5973 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      frame            row_id        type  landmark_index         x         y  \\\n",
       "0        22         22-face-0        face               0  0.452728  0.497625   \n",
       "1        22         22-face-1        face               1  0.444277  0.469700   \n",
       "2        22         22-face-2        face               2  0.446708  0.476862   \n",
       "3        22         22-face-3        face               3  0.438305  0.441203   \n",
       "4        22         22-face-4        face               4  0.444290  0.461555   \n",
       "...     ...               ...         ...             ...       ...       ...   \n",
       "5968     32  32-right_hand-16  right_hand              16  0.259397  0.725355   \n",
       "5969     32  32-right_hand-17  right_hand              17  0.159414  0.684204   \n",
       "5970     32  32-right_hand-18  right_hand              18  0.175069  0.677991   \n",
       "5971     32  32-right_hand-19  right_hand              19  0.188972  0.697104   \n",
       "5972     32  32-right_hand-20  right_hand              20  0.200690  0.713311   \n",
       "\n",
       "             z  \n",
       "0    -0.026592  \n",
       "1    -0.053501  \n",
       "2    -0.027491  \n",
       "3    -0.039765  \n",
       "4    -0.057071  \n",
       "...        ...  \n",
       "5968 -0.272741  \n",
       "5969 -0.181114  \n",
       "5970 -0.243891  \n",
       "5971 -0.263154  \n",
       "5972 -0.274248  \n",
       "\n",
       "[5973 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pf = pd.read_parquet(\"./asl-kaggle/averaged_by_labels/alligator/20165761.parquet\")\n",
    "pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import os\n",
    "from os.path import isfile, join\n",
    "import sys\n",
    "\n",
    "\n",
    "# folders = [\"alligator/\", \"flower/\", \"kiss/\", \"listen/\", \"orange/\"]\n",
    "folders = [\"hello/\", \"all/\", \"thankyou/\", \"for/\", \"time/\", \"will/\", \"now/\", \"please/\", \"quiet/\", \"down/\", \"listen/\", \"close/\", \"have/\", \"time/\", \"no/\", \"nap/\", \"bye/\"]\n",
    "optional = [\"if/\", \"noisy/\", \"mad/\", \"sad/\"]\n",
    "\n",
    "for folder in optional[:]:\n",
    "    path = \"./asl-kaggle/averaged_by_labels/\"+folder\n",
    "    save_path = \"./asl-kaggle/averaged_np_labels/\"+folder\n",
    "    parquets = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "\n",
    "    i = 1\n",
    "    for parquet in parquets[:]:\n",
    "        parquet_path = path+parquet\n",
    "        pf = pd.read_parquet(parquet_path)\n",
    "        \n",
    "        pf = pf.drop(pf.loc[pf.type==\"face\"].index)\n",
    "        pf = pf.fillna(0)\n",
    "\n",
    "        # pf = transform_data(pf, 267)\n",
    "\n",
    "        np_array = convert_parquet_to_np_format(pf)\n",
    "        np_array = np.array(np_array)\n",
    "\n",
    "        save_path2 = save_path+f\"video{i}/\"\n",
    "\n",
    "        if not os.path.exists(save_path2):\n",
    "            os.makedirs(save_path2)\n",
    "\n",
    "        j = 1\n",
    "        for arr in np_array:\n",
    "            np.save(save_path2+f\"{j}\", arr)\n",
    "            j += 1\n",
    "\n",
    "        i += 1\n",
    "\n",
    "        # np.save(save_path+parquet, np_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_table(pf, video_data):\n",
    "    frame_num = 0\n",
    "\n",
    "    for frame in pf.frame.unique():\n",
    "        x_vals = list(pf['x'].loc[pf.frame==frame])\n",
    "        y_vals = list(pf['y'].loc[pf.frame==frame])\n",
    "        z_vals = list(pf['z'].loc[pf.frame==frame])\n",
    "\n",
    "        video_data[f'{frame_num}x'] = x_vals\n",
    "        video_data[f'{frame_num}y'] = y_vals\n",
    "        video_data[f'{frame_num}z'] = z_vals\n",
    "        \n",
    "        frame_num += 1\n",
    "\n",
    "    return video_data\n",
    "\n",
    "def create_data_table(pf):\n",
    "    col_labels = ['type','landmark_index']\n",
    "\n",
    "    for i in range(len(pf.frame.unique())):\n",
    "        col_labels.append(f'{i}x')\n",
    "        col_labels.append(f'{i}y')\n",
    "        col_labels.append(f'{i}z')\n",
    "\n",
    "    landmarks = []\n",
    "    types = []\n",
    "\n",
    "    for i in pf.type.unique():\n",
    "        for j in pf.landmark_index.loc[pf.type==i].unique():\n",
    "            landmarks.append(j)\n",
    "            types.append(i)\n",
    "\n",
    "    data = {col: [0.0] * len(types) for col in col_labels}\n",
    "    data['type'] = types\n",
    "    data['landmark_index'] = landmarks\n",
    "\n",
    "    video_data = pd.DataFrame(columns=col_labels, data=data)\n",
    "    video_data = populate_table(pf, video_data)\n",
    "\n",
    "    return video_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_to_right(pf):\n",
    "    l_non = sum(pf.x.loc[pf.type=='left_hand'] == 0)\n",
    "    r_non = sum(pf.x.loc[pf.type=='right_hand'] == 0)\n",
    "\n",
    "    if l_non > r_non:\n",
    "        pf = pf.drop(pf.x.loc[pf.type=='right_hand'].index, axis=0)\n",
    "        pf.loc[pf.type=='left_hand', 'x'] = 1-pf.x.loc[pf.type=='left_hand']\n",
    "        pf.loc[pf.type=='left_hand', 'type'] = 'right_hand'\n",
    "    # else:\n",
    "        pf = pf.drop(pf.x.loc[pf.type=='left_hand'].index, axis=0)\n",
    "        \n",
    "    return pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = pd.read_parquet(\"./asl-kaggle/averaged_by_labels/alligator/20165761.parquet\")\n",
    "pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from scipy.stats import skew, kurtosis, entropy\n",
    "import sys\n",
    "\n",
    "\n",
    "folders = [\"alligator/\", \"flower/\", \"kiss/\", \"listen/\", \"orange/\"]\n",
    "aggregated_files = []\n",
    "\n",
    "for folder in folders:\n",
    "    path = \"./asl-kaggle/averaged_by_labels/\"+folder\n",
    "    # path = \"./asl-kaggle/by_labels/\"+folder\n",
    "    parquets = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "\n",
    "    for parquet in parquets[:50]:\n",
    "        parquet_path = path+parquet\n",
    "        pf = pd.read_parquet(parquet_path)\n",
    "        \n",
    "        pf = pf.drop(pf.loc[pf.type==\"face\"].index)\n",
    "        pf = pf.fillna(0)\n",
    "\n",
    "        # pf = default_to_right(pf)\n",
    "        pf = transform_data(pf, 30)\n",
    "        video_data = create_data_table(pf)\n",
    "\n",
    "        aggregated_row = np.array([folder])\n",
    "        # aggregated_row = np.append(aggregated_row, video_data.drop(['type','landmark_index'], axis=1).agg(['mean', 'median', 'min', 'max']).values.flatten())\n",
    "        # Drop unnecessary columns\n",
    "        dropped_columns = video_data.drop(['type', 'landmark_index'], axis=1)\n",
    "\n",
    "        # Calculate mean, median, min, max for each column\n",
    "        basic_stats = dropped_columns.agg(['mean', 'median', 'min', 'max']).values.flatten()\n",
    "\n",
    "        # Calculate skew and kurtosis for each column\n",
    "        skew_kurtosis_stats = dropped_columns.apply(lambda x: pd.Series([skew(x), kurtosis(x)])).values.flatten()\n",
    "        \n",
    "        # Concatenate all the calculated values\n",
    "        aggregated_values = np.concatenate([basic_stats, skew_kurtosis_stats])\n",
    "\n",
    "        # Append the calculated values to aggregated_row\n",
    "        aggregated_row = np.append(aggregated_row, aggregated_values)\n",
    "\n",
    "        aggregated_files.append(aggregated_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_pf = pd.DataFrame(aggregated_files)\n",
    "aggregated_pf = aggregated_pf.rename(columns={0: 'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nas = 0\n",
    "\n",
    "# for i in aggregated_pf.columns:\n",
    "#     nas += sum(aggregated_pf[i].isna())\n",
    "\n",
    "# nas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn.metrics as sk_metrics\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Preset matplotlib figure sizes.\n",
    "matplotlib.rcParams['figure.figsize'] = [9, 6]\n",
    "\n",
    "print(tf.__version__)\n",
    "# To make the results reproducible, set the random seed value.\n",
    "tf.random.set_seed(22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize(tf.Module):\n",
    "  def __init__(self, x):\n",
    "    # Initialize the mean and standard deviation for normalization\n",
    "    self.mean = tf.Variable(tf.math.reduce_mean(x, axis=0))\n",
    "    self.std = tf.Variable(tf.math.reduce_std(x, axis=0)) + 0.001\n",
    "\n",
    "  def norm(self, x):\n",
    "    # Normalize the input\n",
    "    return (x - self.mean)/self.std\n",
    "\n",
    "  def unnorm(self, x):\n",
    "    # Unnormalize the input\n",
    "    return (x * self.std) + self.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def reduce_dims(n_components, x_train, x_test):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(x_train)\n",
    "    return pca.transform(x_train), pca.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "scores = []\n",
    "\n",
    "for i in range(1, 21):\n",
    "    r_state = random.randint(1,1000)\n",
    "    train_dataset = aggregated_pf.sample(frac=0.75, random_state=r_state)\n",
    "    test_dataset = aggregated_pf.drop(train_dataset.index)\n",
    "\n",
    "    x_train, y_train = train_dataset.iloc[:, 1:], train_dataset.iloc[:, 0]\n",
    "    x_test, y_test = test_dataset.iloc[:, 1:], test_dataset.iloc[:, 0]\n",
    "\n",
    "    x_train = tf.convert_to_tensor(x_train, dtype=tf.float32)\n",
    "    x_test = tf.convert_to_tensor(x_test, dtype=tf.float32)\n",
    "\n",
    "    norm_x = Normalize(x_train)\n",
    "    x_train_norm, x_test_norm = norm_x.norm(x_train), norm_x.norm(x_test)\n",
    "\n",
    "    # pca_x_train, pca_x_test = reduce_dims(40, x_train_norm, x_test_norm)\n",
    "    pca_x_train, pca_x_test = x_train_norm, x_test_norm\n",
    "\n",
    "    classif = LogisticRegression(random_state=0, max_iter=10000).fit(pca_x_train, y_train)\n",
    "    # classif = RandomForestClassifier().fit(pca_x_train, y_train)\n",
    "    # classif = SVC(gamma='auto').fit(pca_x_train, y_train)\n",
    "    \n",
    "    y_preds = classif.predict(pca_x_test)\n",
    "    scores.append(accuracy_score(y_test, y_preds))\n",
    "\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With augmented points dataset :-\n",
    "#   LogisticRegression - Best score at pca_comps = 30 - score = 0.439\n",
    "#   RandomForest - Best score at pca_comps = 10 - score = 0.458\n",
    "#   SVC - Best score at pca_comps = 70 - score = 0.405"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code source: Gaël Varoquaux\n",
    "#              Andreas Müller\n",
    "# Modified for documentation by Jaques Grobler\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "names = [\n",
    "    \"Nearest Neighbors\",\n",
    "    \"Linear SVM\",\n",
    "    \"RBF SVM\",\n",
    "    \"Gaussian Process\",\n",
    "    \"Decision Tree\",\n",
    "    \"Random Forest\",\n",
    "    \"Neural Net\",\n",
    "    \"AdaBoost\",\n",
    "    \"Naive Bayes\",\n",
    "    \"QDA\",\n",
    "    \"Logistic Regression\",\n",
    "]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025, random_state=42),\n",
    "    SVC(gamma=2, C=1, random_state=42),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0), random_state=42),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    MLPClassifier(alpha=1, max_iter=10000, random_state=42),\n",
    "    AdaBoostClassifier(algorithm=\"SAMME\", random_state=42),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "    LogisticRegression(max_iter=10000, random_state=42),\n",
    "]\n",
    "\n",
    "os.environ[\"LOKY_MAX_CPU_COUNT\"] = \"12\"\n",
    "\n",
    "scores_per_classif = dict(zip(names, [[] for i in range(len(names))]))\n",
    "\n",
    "for i in range(20):\n",
    "    r_state = random.randint(1,100)\n",
    "    train_dataset = aggregated_pf.sample(frac=0.75, random_state=r_state)\n",
    "    test_dataset = aggregated_pf.drop(train_dataset.index)\n",
    "\n",
    "    x_train, y_train = train_dataset.iloc[:, 1:], train_dataset.iloc[:, 0]\n",
    "    x_test, y_test = test_dataset.iloc[:, 1:], test_dataset.iloc[:, 0]\n",
    "\n",
    "    x_train = tf.convert_to_tensor(x_train, dtype=tf.float32)\n",
    "    x_test = tf.convert_to_tensor(x_test, dtype=tf.float32)\n",
    "\n",
    "    norm_x = Normalize(x_train)\n",
    "    x_train_norm, x_test_norm = norm_x.norm(x_train), norm_x.norm(x_test)\n",
    "\n",
    "    pca_x_train, pca_x_test = reduce_dims(80, x_train_norm, x_test_norm)\n",
    "    # pca_x_train, pca_x_test = x_train_norm, x_test_norm\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        classif = clf\n",
    "        classif.fit(pca_x_train, y_train)\n",
    "        y_pred = classif.predict(pca_x_test)\n",
    "\n",
    "        scores_per_classif[name].append(accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(scores_per_classif).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(pd.DataFrame(scores_per_classif).mean().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IS460MLApps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
