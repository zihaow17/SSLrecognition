{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\zorko\\anaconda3\\envs\\IS460MLApps\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"d:\\\\SMU\\\\ml&applns\")\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from Preprocessing.DataExtration import *\n",
    "from Preprocessing.DFTransformations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_PCA(n_components, x_train, x_test):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(x_train)\n",
    "    return pca.transform(x_train), pca.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize(tf.Module):\n",
    "  def __init__(self, x):\n",
    "    # Initialize the mean and standard deviation for normalization\n",
    "    self.mean = tf.Variable(tf.math.reduce_mean(x, axis=0))\n",
    "    self.std = tf.Variable(tf.math.reduce_std(x, axis=0)) + 0.001\n",
    "\n",
    "  def norm(self, x):\n",
    "    # Normalize the input\n",
    "    return (x - self.mean)/self.std\n",
    "\n",
    "  def unnorm(self, x):\n",
    "    # Unnormalize the input\n",
    "    return (x * self.std) + self.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_stats_types(pf):\n",
    "    grouped_data = pf.groupby('frame')[['x','y','z']]\n",
    "    grouped_data = np.array([group.values for _, group in grouped_data])\n",
    "    arr = np.transpose(grouped_data, (0, 2, 1))\n",
    "\n",
    "    face_data = arr[:, :, :7]\n",
    "    left_hand_data = arr[:, :, 7:24]\n",
    "    pose_data = arr[:, :, 24:30]\n",
    "    right_hand_data = arr[:, :, 30:]\n",
    "\n",
    "    data = [left_hand_data, pose_data, right_hand_data]\n",
    "\n",
    "    video_data = calculate_stats_np(face_data)\n",
    "\n",
    "    for a in data:\n",
    "        video_data = np.concatenate((video_data, calculate_stats_np(a)), axis=1)\n",
    "\n",
    "    video_data = np.nan_to_num(video_data, 0)\n",
    "\n",
    "    return video_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Initialize a list of folder names containing parquet files\n",
    "folders = [\"hello\", \"all\", \"thankyou\", \"for\", \"time\", \"will\", \"now\", \"please\", \"quiet\", \"down\", \"listen\", \"close\", \"have\", \"no\", \"nap\", \"bye\", \"base/\", \"good/\", \"how/\", \"show/\", \"we/\", \"work/\"]\n",
    "optional = [\"if/\", \"noisy/\", \"mad/\", \"sad/\"]\n",
    "\n",
    "folders = np.append(folders, optional)\n",
    "\n",
    "# Initialize an empty list for all \n",
    "aggregated_files = []\n",
    "labels = []\n",
    "\n",
    "# Iterate over the folders in the list\n",
    "for folder in folders:\n",
    "    \n",
    "    # Update path to focus on content inside folder in current iteration\n",
    "    path = \"./asl-kaggle/averaged_by_labels/\"+folder+\"/\"\n",
    "    # Fetch all files names in folder\n",
    "    parquets = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "\n",
    "    # Iterate over file names in list\n",
    "    # Iterate up to the 50th file name\n",
    "    for parquet in parquets[:]:\n",
    "\n",
    "        # Update path to focus on file in current iteration\n",
    "        parquet_path = path+parquet\n",
    "        # Read the file at path and load data to pf\n",
    "        pf = pd.read_parquet(parquet_path)\n",
    "        \n",
    "        # Remove all rows with type of face\n",
    "        pf = pf.drop(pf.loc[pf.type==\"face\"].index)\n",
    "        # Replace all Nan values with 0\n",
    "        pf = pf.fillna(0)\n",
    "\n",
    "        if len(pf.frame.unique()) < 10:\n",
    "            continue\n",
    "\n",
    "## ------------------------ To Tune ------------------------ ##\n",
    "\n",
    "        # Transform data in pf to accomodate desired frame amount\n",
    "        # If less than frame num, duplicate frames\n",
    "        # If more than frame num, delete frames\n",
    "        pf = transform_data(pf, 65, True, True) # (int)\n",
    "\n",
    "## --------------------------------------------------------- ##\n",
    "        \n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        video_data = calc_stats_types(pf)\n",
    "        video_data = video_data.reshape((-1))\n",
    "\n",
    "        aggregated_files.append(video_data)\n",
    "        labels.append(folder)\n",
    "\n",
    "# Convert list of file measures into a pandas dataframe\n",
    "final_dataset = pd.DataFrame(aggregated_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28309305373525556\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list for accuracy scores\n",
    "scores = []\n",
    "\n",
    "# Loop 20 times\n",
    "for i in range(1):\n",
    "    # Split dataset and labels into train and test sets\n",
    "    # Split ratio defined by test_size parameter (0.3 means 30% of total data is assigned to test set)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(final_dataset, labels, test_size=0.2, shuffle=True)\n",
    "\n",
    "    # Convert x_train and x_test into tensorflow tensors\n",
    "    x_train = tf.convert_to_tensor(x_train, dtype=tf.float32)\n",
    "    x_test = tf.convert_to_tensor(x_test, dtype=tf.float32)\n",
    "\n",
    "    # Normalize the train and test data\n",
    "    norm_x = Normalize(x_train)\n",
    "    x_train_norm, x_test_norm = norm_x.norm(x_train), norm_x.norm(x_test)\n",
    "\n",
    "    # Apply PCA to data sets\n",
    "    pca_x_train, pca_x_test = apply_PCA(60, x_train_norm, x_test_norm)\n",
    "    # pca_x_train, pca_x_test = x_train_norm, x_test_norm\n",
    "\n",
    "## ------------------------ To Tune ------------------------ ##\n",
    "\n",
    "    # Initialize Logistic Regression and train on training set\n",
    "    classif = LogisticRegression(\n",
    "        penalty = 'l2', # (l1, l2, elasticnet, None)\n",
    "        dual = False, # (False, True)\n",
    "        tol = 1e-7, # (1e-4 - 1e-7)\n",
    "        C = 1.0, # (positive float)\n",
    "        class_weight = None, # (None, default)\n",
    "        solver = 'lbfgs', # (lbfgs, liblinear, newton-cg, newton-cholesky, sag, saga)\n",
    "        random_state = None, # (None, int)\n",
    "        verbose = 0, # (int)\n",
    "        l1_ratio = None, # (None, float from 0 to 1)\n",
    "        max_iter = 100000 # (Don't change unless warnings)\n",
    "    ).fit(pca_x_train, y_train)\n",
    "\n",
    "## --------------------------------------------------------- ##\n",
    "\n",
    "    # classif = RandomForestClassifier().fit(pca_x_train, y_train)\n",
    "    # classif = SVC(gamma='auto').fit(pca_x_train, y_train)\n",
    "    classif = DecisionTreeClassifier().fit(pca_x_train, y_train)\n",
    "    \n",
    "    # Predict labels for test set\n",
    "    y_preds = classif.predict(pca_x_test)\n",
    "    # Calculate accuracy score of predictions and append to list\n",
    "    scores.append(accuracy_score(y_test, y_preds))\n",
    "\n",
    "# Calculate the mean accuracy score across all iterations\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Ignore Below, unless you feel confident to try other models__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u> __Welcome brave one!__ </u>\n",
    "\n",
    "\n",
    "For available parameters to tune follow the following URLs for info:\n",
    "\n",
    "KNeighbours : https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "\n",
    "SVM : https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "\n",
    "GaussianProcess : https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessClassifier.html\n",
    "\n",
    "Decision Tree : https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "\n",
    "Random Forest : https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "Neural Net : https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "\n",
    "AdaBoost : https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n",
    "\n",
    "Naive Bayes : https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html\n",
    "\n",
    "QDA : https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick Big Tip!!\n",
    "\n",
    "Do not go tuning every single classifier plz!\n",
    "\n",
    "I would suggest focusing on the following:\n",
    "\n",
    "- SVM or SVC (depends which name you like)\n",
    "- Random Forest\n",
    "- Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code source: Gaël Varoquaux\n",
    "#              Andreas Müller\n",
    "# Modified for documentation by Jaques Grobler\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import os\n",
    "\n",
    "# !! To reduce amount of classifiers tested/used, remove them from the below lists !!\n",
    "# i.e. the list names and the list classifiers\n",
    "# This is if you want to focus on specific models, but don't want to waste time on training and testing other models\n",
    "names = [\n",
    "    \"Nearest Neighbors\",\n",
    "    \"Linear SVM\",\n",
    "    \"RBF SVM\",\n",
    "    # \"Gaussian Process\",\n",
    "    \"Decision Tree\",\n",
    "    \"Random Forest\",\n",
    "    \"Neural Net\",\n",
    "    \"AdaBoost\",\n",
    "    \"Naive Bayes\",\n",
    "    \"QDA\",\n",
    "    \"Logistic Regression\",\n",
    "]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025, random_state=42),\n",
    "    SVC(gamma=2, C=1, random_state=42),\n",
    "    # GaussianProcessClassifier(1.0 * RBF(1.0), random_state=42),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    MLPClassifier(alpha=1, max_iter=10000, random_state=42),\n",
    "    AdaBoostClassifier(algorithm=\"SAMME\", random_state=42),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "    LogisticRegression(max_iter=10000, random_state=42),\n",
    "]\n",
    "\n",
    "# Got no idea how many logical processors you have\n",
    "# From Monday's talk you said 8 cores, so I set it to 8\n",
    "# If you are able to find out number of logical cores in your laptop or if your cpu supports hyperthreading, change the value to 16 :)\n",
    "os.environ[\"LOKY_MAX_CPU_COUNT\"] = \"6\"\n",
    "\n",
    "# Initialize a dictionary of lists\n",
    "# This will be used to store the accuracy scores of each model for each iteration\n",
    "scores_per_classif = dict(zip(names, [[] for i in range(len(names))]))\n",
    "\n",
    "# Loop 20 times\n",
    "for i in range(5):\n",
    "    # Split dataset and labels into train and test sets\n",
    "    # Split ratio defined by test_size parameter (0.3 means 30% of total data is assigned to test set)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(final_dataset, labels, test_size=0.2, shuffle=True)\n",
    "\n",
    "    # Convert x_train and x_test into tensorflow tensors\n",
    "    x_train = tf.convert_to_tensor(x_train, dtype=tf.float32)\n",
    "    x_test = tf.convert_to_tensor(x_test, dtype=tf.float32)\n",
    "\n",
    "    # Normalize the train and test data\n",
    "    norm_x = Normalize(x_train)\n",
    "    x_train_norm, x_test_norm = norm_x.norm(x_train), norm_x.norm(x_test)\n",
    "\n",
    "    # Apply PCA to data sets\n",
    "    pca_x_train, pca_x_test = apply_PCA(60, x_train_norm, x_test_norm)\n",
    "    # pca_x_train, pca_x_test = x_train_norm, x_test_norm\n",
    "\n",
    "    # Iterate over classifiers and their names\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        # Train model on train data\n",
    "        clf.fit(pca_x_train, y_train)\n",
    "        # Predict labels of test set\n",
    "        y_pred = clf.predict(pca_x_test)\n",
    "\n",
    "        # Calculate accuracy score and append to list in dictionary at key = name of classifier\n",
    "        scores_per_classif[name].append(accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Nearest Neighbors      0.313499\n",
       "Linear SVM             0.306422\n",
       "RBF SVM                0.058847\n",
       "Decision Tree          0.272215\n",
       "Random Forest          0.347706\n",
       "Neural Net             0.387287\n",
       "AdaBoost               0.183879\n",
       "Naive Bayes            0.209699\n",
       "QDA                    0.282307\n",
       "Logistic Regression    0.300786\n",
       "dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output mean accuracy scores for each model\n",
    "pd.DataFrame(scores_per_classif).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IS460MLApps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
