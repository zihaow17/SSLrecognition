{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"d:\\\\SMU\\\\ml&applns\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from scipy.stats import skew, kurtosis, entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_row_id(row):\n",
    "    return str(row.frame) + \"-\" + row.type + \"-\" + str(row.landmark_index)\n",
    "\n",
    "def duplicate_vals(pf, values: np.array, iter):\n",
    "    frames_to_dup = pf.loc[pf.frame.isin(values)].copy()\n",
    "    frames_to_dup.frame += (frames_to_dup.frame-round(frames_to_dup.frame))*0.001 + 0.01*iter\n",
    "    \n",
    "    frames_to_dup.row_id = frames_to_dup.apply(get_row_id, axis=1)\n",
    "\n",
    "    pf = pd.concat([pf, frames_to_dup], ignore_index=True).sort_values('frame')\n",
    "\n",
    "    return pf\n",
    "\n",
    "def remove_vals(pf, values: np.array):\n",
    "    values = np.sort(values)\n",
    "\n",
    "    for val in values:\n",
    "        pf = pf.loc[pf.frame != val]\n",
    "\n",
    "    return pf\n",
    "\n",
    "\n",
    "def transform_data(pf, frame_amt_goal, iter=1):\n",
    "\n",
    "    frame_nums = pf.frame.unique()\n",
    "    frame_diff = abs(frame_amt_goal - len(frame_nums))\n",
    "    operation = frame_amt_goal > len(frame_nums)\n",
    "\n",
    "    values_to_operate = np.array([])\n",
    "\n",
    "    if frame_diff%2 == 1:\n",
    "        central_point = frame_nums[int(len(frame_nums)/2)]\n",
    "        values_to_operate = np.append(values_to_operate, [central_point])\n",
    "        frame_nums = np.delete(frame_nums, [int(len(frame_nums)/2)])\n",
    "        frame_diff -= 1\n",
    "\n",
    "    if frame_diff != 0:\n",
    "        step_val = len(frame_nums)/frame_diff\n",
    "        step_val = 1 if step_val < 1 else step_val\n",
    "        \n",
    "        loop_cnt = len(frame_nums) if frame_diff > len(frame_nums) else frame_diff\n",
    "        values_to_operate = np.append(values_to_operate, frame_nums[[int(i*step_val) for i in range(0, loop_cnt)]])\n",
    "    else:\n",
    "        loop_cnt = 0\n",
    "\n",
    "    if operation:\n",
    "        pf = duplicate_vals(pf, values_to_operate, iter)\n",
    "    else:\n",
    "        pf = remove_vals(pf, values_to_operate)\n",
    "\n",
    "    if frame_diff - loop_cnt != 0:\n",
    "        pf = transform_data(pf, frame_amt_goal, iter+1)\n",
    "\n",
    "    return pf\n",
    "\n",
    "def populate_table(pf, video_data):\n",
    "    frame_num = 0\n",
    "\n",
    "    for frame in pf.frame.unique():\n",
    "        x_vals = list(pf['x'].loc[pf.frame==frame])\n",
    "        y_vals = list(pf['y'].loc[pf.frame==frame])\n",
    "        z_vals = list(pf['z'].loc[pf.frame==frame])\n",
    "\n",
    "        video_data[f'{frame_num}x'] = x_vals\n",
    "        video_data[f'{frame_num}y'] = y_vals\n",
    "        video_data[f'{frame_num}z'] = z_vals\n",
    "        \n",
    "        frame_num += 1\n",
    "\n",
    "    return video_data\n",
    "\n",
    "def create_data_table(pf):\n",
    "    col_labels = ['type','landmark_index']\n",
    "\n",
    "    for i in range(len(pf.frame.unique())):\n",
    "        col_labels.append(f'{i}x')\n",
    "        col_labels.append(f'{i}y')\n",
    "        col_labels.append(f'{i}z')\n",
    "\n",
    "    landmarks = []\n",
    "    types = []\n",
    "\n",
    "    for i in pf.type.unique():\n",
    "        for j in pf.landmark_index.loc[pf.type==i].unique():\n",
    "            landmarks.append(j)\n",
    "            types.append(i)\n",
    "\n",
    "    data = {col: [0.0] * len(types) for col in col_labels}\n",
    "    data['type'] = types\n",
    "    data['landmark_index'] = landmarks\n",
    "\n",
    "    video_data = pd.DataFrame(columns=col_labels, data=data)\n",
    "    video_data = populate_table(pf, video_data)\n",
    "\n",
    "    return video_data\n",
    "\n",
    "def apply_PCA(n_components, x_train, x_test):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(x_train)\n",
    "    return pca.transform(x_train), pca.transform(x_test)\n",
    "\n",
    "def drop_empty_rows(pf):\n",
    "    pf = pf.drop(pf.loc[(pf.x == 0) & (pf.y == 0) & (pf.z == 0)].index, axis=0)\n",
    "    return pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize(tf.Module):\n",
    "  def __init__(self, x):\n",
    "    # Initialize the mean and standard deviation for normalization\n",
    "    self.mean = tf.Variable(tf.math.reduce_mean(x, axis=0))\n",
    "    self.std = tf.Variable(tf.math.reduce_std(x, axis=0)) + 0.001\n",
    "\n",
    "  def norm(self, x):\n",
    "    # Normalize the input\n",
    "    return (x - self.mean)/self.std\n",
    "\n",
    "  def unnorm(self, x):\n",
    "    # Unnormalize the input\n",
    "    return (x * self.std) + self.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = pd.read_parquet(\"./asl-kaggle/averaged_by_labels/alligator/20165761.parquet\")\n",
    "pf = pf.fillna(0)\n",
    "pf = pf.drop(pf.loc[pf.type==\"face\"].index).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "types = ['pose', 'left_hand', 'right_hand']\n",
    "\n",
    "ohe = LabelBinarizer()\n",
    "ohe.fit(types)\n",
    "transformed = ohe.transform(pf.type)\n",
    "ohe_df = pd.DataFrame(transformed)\n",
    "data = pd.concat([pf, ohe_df], axis=1).drop(['type'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hot_encode(encoder, df):\n",
    "    encoded_data = encoder.transform(df.type)\n",
    "    encoded_df = pd.DataFrame(encoded_data)\n",
    "    df = pd.concat([df, encoded_df], axis=1).drop(['type'], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import skew, kurtosis, entropy\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "types = ['pose', 'left_hand', 'right_hand']\n",
    "lblBin = LabelBinarizer().fit(types)\n",
    "\n",
    "# Initialize a list of folder names containing parquet files\n",
    "folders = [\"alligator\", \"flower\", \"kiss\", \"listen\", \"orange\"]\n",
    "\n",
    "# Initialize lists for aggregated data and labels\n",
    "aggregated_files = []\n",
    "labels = []\n",
    "\n",
    "# Iterate over the folders in the list\n",
    "for folder in folders:\n",
    "    # Update path to focus on content inside folder in the current iteration\n",
    "    path = \"./asl-kaggle/by_labels/\"+folder+\"/\"\n",
    "\n",
    "    # Fetch all file names in the folder\n",
    "    parquets = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "\n",
    "    # Iterate over file names in the list (up to the 50th file name)\n",
    "    for parquet in parquets[:50]:\n",
    "        # Update path to focus on the file in the current iteration\n",
    "        parquet_path = path + parquet\n",
    "        \n",
    "        # Read the file at the path and load data to pf\n",
    "        pf = pd.read_parquet(parquet_path)\n",
    "        \n",
    "        # Replace all NaN values with 0\n",
    "        pf = pf.fillna(0)\n",
    "\n",
    "        # Remove all rows with the type of face\n",
    "        pf = pf.drop(pf.loc[pf.type==\"face\"].index).reset_index(drop=True)\n",
    "        pf = pf.drop(['row_id'], axis=1)\n",
    "\n",
    "        pf = hot_encode(lblBin, pf)\n",
    "\n",
    "        frames_data = pf.groupby('frame').apply(lambda group: group.drop('frame', axis=1).to_numpy().tolist())\n",
    "        video_data = np.array(frames_data.tolist())\n",
    "\n",
    "        # Append array to list\n",
    "        aggregated_files.append(video_data)\n",
    "        labels.append(folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(aggregated_files[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_files[249].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum length of arrays\n",
    "max_length = max(arr.shape[0] for arr in aggregated_files)\n",
    "\n",
    "# Pad each array along the first dimension (rows)\n",
    "padded_data = [np.pad(arr, ((0, max_length - arr.shape[0]), (0, 0), (0, 0)), mode='constant') for arr in aggregated_files]\n",
    "\n",
    "# Convert the list of padded arrays back to a numpy array\n",
    "padded_data_array = np.array(padded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_data_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IS460MLApps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
