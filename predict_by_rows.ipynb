{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\zorko\\anaconda3\\envs\\IS460MLApps\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_row_id(row):\n",
    "    return str(row.frame) + \"-\" + row.type + \"-\" + str(row.landmark_index)\n",
    "\n",
    "def duplicate_vals(pf, values: np.array, iter):\n",
    "    frames_to_dup = pf.loc[pf.frame.isin(values)].copy()\n",
    "    frames_to_dup.frame += (frames_to_dup.frame-round(frames_to_dup.frame))*0.001 + 0.01*iter\n",
    "    \n",
    "    frames_to_dup.row_id = frames_to_dup.apply(get_row_id, axis=1)\n",
    "\n",
    "    pf = pd.concat([pf, frames_to_dup], ignore_index=True).sort_values('frame')\n",
    "\n",
    "    return pf\n",
    "\n",
    "def remove_vals(pf, values: np.array):\n",
    "    values = np.sort(values)\n",
    "\n",
    "    for val in values:\n",
    "        pf = pf.loc[pf.frame != val]\n",
    "\n",
    "    return pf\n",
    "\n",
    "\n",
    "def transform_data(pf, frame_amt_goal, iter=1):\n",
    "\n",
    "    frame_nums = pf.frame.unique()\n",
    "    frame_diff = abs(frame_amt_goal - len(frame_nums))\n",
    "    operation = frame_amt_goal > len(frame_nums)\n",
    "\n",
    "    values_to_operate = np.array([])\n",
    "\n",
    "    if frame_diff%2 == 1:\n",
    "        central_point = frame_nums[int(len(frame_nums)/2)]\n",
    "        values_to_operate = np.append(values_to_operate, [central_point])\n",
    "        frame_nums = np.delete(frame_nums, [int(len(frame_nums)/2)])\n",
    "        frame_diff -= 1\n",
    "\n",
    "    if frame_diff != 0:\n",
    "        step_val = len(frame_nums)/frame_diff\n",
    "        step_val = 1 if step_val < 1 else step_val\n",
    "        \n",
    "        loop_cnt = len(frame_nums) if frame_diff > len(frame_nums) else frame_diff\n",
    "        values_to_operate = np.append(values_to_operate, frame_nums[[int(i*step_val) for i in range(0, loop_cnt)]])\n",
    "    else:\n",
    "        loop_cnt = 0\n",
    "\n",
    "    if operation:\n",
    "        pf = duplicate_vals(pf, values_to_operate, iter)\n",
    "    else:\n",
    "        pf = remove_vals(pf, values_to_operate)\n",
    "\n",
    "    if frame_diff - loop_cnt != 0:\n",
    "        pf = transform_data(pf, frame_amt_goal, iter+1)\n",
    "\n",
    "    return pf\n",
    "\n",
    "def populate_table(pf, video_data):\n",
    "    frame_num = 0\n",
    "\n",
    "    for frame in pf.frame.unique():\n",
    "        x_vals = list(pf['x'].loc[pf.frame==frame])\n",
    "        y_vals = list(pf['y'].loc[pf.frame==frame])\n",
    "        z_vals = list(pf['z'].loc[pf.frame==frame])\n",
    "\n",
    "        video_data[f'{frame_num}x'] = x_vals\n",
    "        video_data[f'{frame_num}y'] = y_vals\n",
    "        video_data[f'{frame_num}z'] = z_vals\n",
    "        \n",
    "        frame_num += 1\n",
    "\n",
    "    return video_data\n",
    "\n",
    "def create_data_table(pf):\n",
    "    col_labels = ['type','landmark_index']\n",
    "\n",
    "    for i in range(len(pf.frame.unique())):\n",
    "        col_labels.append(f'{i}x')\n",
    "        col_labels.append(f'{i}y')\n",
    "        col_labels.append(f'{i}z')\n",
    "\n",
    "    landmarks = []\n",
    "    types = []\n",
    "\n",
    "    for i in pf.type.unique():\n",
    "        for j in pf.landmark_index.loc[pf.type==i].unique():\n",
    "            landmarks.append(j)\n",
    "            types.append(i)\n",
    "\n",
    "    data = {col: [0.0] * len(types) for col in col_labels}\n",
    "    data['type'] = types\n",
    "    data['landmark_index'] = landmarks\n",
    "\n",
    "    video_data = pd.DataFrame(columns=col_labels, data=data)\n",
    "    video_data = populate_table(pf, video_data)\n",
    "\n",
    "    return video_data\n",
    "\n",
    "def drop_empty_rows(pf):\n",
    "    pf = pf.drop(pf.loc[(pf.x == 0) & (pf.y == 0) & (pf.z == 0)].index, axis=0)\n",
    "    return pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [\"alligator/\", \"flower/\", \"kiss/\", \"listen/\", \"orange/\"]\n",
    "aggregated_pf = pd.DataFrame()\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(['right_hand','left_hand','pose'])\n",
    "\n",
    "file_id = 0\n",
    "unique_ids = random.sample(range(1000, 10000), 50*len(folders))\n",
    "\n",
    "for folder in folders:\n",
    "    path = \"./asl-kaggle/averaged_by_labels/\"+folder\n",
    "    # path = \"./asl-kaggle/by_labels/\"+folder\n",
    "    parquets = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "\n",
    "    for parquet in parquets[:50]:\n",
    "        parquet_path = path+parquet\n",
    "        pf = pd.read_parquet(parquet_path)\n",
    "        \n",
    "        pf = pf.drop(pf.loc[pf.type==\"face\"].index)\n",
    "        pf = pf.fillna(0)\n",
    "\n",
    "        pf = transform_data(pf, 30)\n",
    "        pf = drop_empty_rows(pf)\n",
    "        video_data = create_data_table(pf)\n",
    "        video_data['type'] = le.transform(video_data['type'])\n",
    "        video_data['label'] = folder\n",
    "        video_data['file_id'] = file_id\n",
    "\n",
    "        aggregated_pf = pd.concat([aggregated_pf, video_data])\n",
    "\n",
    "        file_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tf.__version__)\n",
    "# # To make the results reproducible, set the random seed value.\n",
    "# tf.random.set_seed(22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize(tf.Module):\n",
    "  def __init__(self, x):\n",
    "    # Initialize the mean and standard deviation for normalization\n",
    "    self.mean = tf.Variable(tf.math.reduce_mean(x, axis=0))\n",
    "    self.std = tf.Variable(tf.math.reduce_std(x, axis=0)) + 0.001\n",
    "\n",
    "  def norm(self, x):\n",
    "    # Normalize the input\n",
    "    return (x - self.mean)/self.std\n",
    "\n",
    "  def unnorm(self, x):\n",
    "    # Unnormalize the input\n",
    "    return (x * self.std) + self.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9785714285714286\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "# Iterate x amount of times to get an average accuracy score\n",
    "for i in range(20):\n",
    "    # Retrieve file ids\n",
    "    ids = aggregated_pf['file_id'].unique()\n",
    "    # Calculate the numerical value of 75% of ids\n",
    "    num_elements_to_select = int(0.75 * len(ids))\n",
    "    # Choose at random 75% of ids from dataset for train test\n",
    "    train_values = np.random.choice(ids, size=num_elements_to_select, replace=False)\n",
    "    # Remaining ids assign to test set\n",
    "    test_values = ids[~np.isin(ids, train_values)]\n",
    "\n",
    "    # Retrieve data from dataset where file_id is in list of train values\n",
    "    train_dataset = aggregated_pf.loc[(aggregated_pf.file_id.isin(train_values))]\n",
    "    # Remove the label from train_dataset and assign to x_test\n",
    "    # Retrieve labels and assign to y_train\n",
    "    x_train, y_train = train_dataset.drop(['label'], axis=1), train_dataset['label']\n",
    "\n",
    "    # Covnert dataframe to tensor\n",
    "    x_train = tf.convert_to_tensor(x_train, dtype=tf.float32)\n",
    "\n",
    "    # Normalize train set\n",
    "    norm_x = Normalize(x_train)\n",
    "    x_train_norm = norm_x.norm(x_train)\n",
    "\n",
    "    # Fit train data\n",
    "    classif = LogisticRegression(random_state=0, max_iter=10000).fit(x_train_norm, y_train)\n",
    "    # classif = RandomForestClassifier().fit(pca_x_train, y_train)\n",
    "    # classif = SVC(gamma='auto').fit(pca_x_train, y_train)\n",
    "\n",
    "    # Initialize list of actual labels and predicitions\n",
    "    y_test = []\n",
    "    y_preds = []\n",
    "\n",
    "    # Loop over file_ids in test_values\n",
    "    for id in test_values:\n",
    "        # Append actual label of data at file id\n",
    "        y_test.append(aggregated_pf.label.loc[(aggregated_pf.file_id == id)].unique()[0])\n",
    "\n",
    "        # Retrieve data for provided file id and drop label\n",
    "        x_test = aggregated_pf.loc[(aggregated_pf.file_id == id)].drop(['label'], axis=1)\n",
    "        # Convert test data to tensor\n",
    "        x_test = tf.convert_to_tensor(x_test, dtype=tf.float32)\n",
    "        # Normalize test data\n",
    "        x_test_norm =  norm_x.norm(x_test)\n",
    "\n",
    "        # Predict labels for each row in dataset\n",
    "        pred = classif.predict(x_test_norm)\n",
    "        # Initialize a counter object to find most common label\n",
    "        counter = Counter(pred)\n",
    "        # Find most common label in predictions\n",
    "        pred_label = counter.most_common(1)[0][0]\n",
    "        # Append predicted label to list of predictions\n",
    "        y_preds.append(pred_label)\n",
    "\n",
    "    # Calculate accuracy for test set and append to list of scores\n",
    "    scores.append(accuracy_score(y_preds, y_test))\n",
    "\n",
    "# Find the average score for all iterations\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IS460MLApps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
