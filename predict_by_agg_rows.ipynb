{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\zorko\\anaconda3\\envs\\IS460MLApps\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import skew, kurtosis\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_row_id(row):\n",
    "    return str(row.frame) + \"-\" + row.type + \"-\" + str(row.landmark_index)\n",
    "\n",
    "def duplicate_vals(pf, values: np.array, iter):\n",
    "    frames_to_dup = pf.loc[pf.frame.isin(values)].copy()\n",
    "    frames_to_dup.frame += (frames_to_dup.frame-round(frames_to_dup.frame))*0.001 + 0.01*iter\n",
    "    \n",
    "    frames_to_dup.row_id = frames_to_dup.apply(get_row_id, axis=1)\n",
    "\n",
    "    pf = pd.concat([pf, frames_to_dup], ignore_index=True).sort_values('frame')\n",
    "\n",
    "    return pf\n",
    "\n",
    "def remove_vals(pf, values: np.array):\n",
    "    values = np.sort(values)\n",
    "\n",
    "    for val in values:\n",
    "        pf = pf.loc[pf.frame != val]\n",
    "\n",
    "    return pf\n",
    "\n",
    "\n",
    "def transform_data(pf, frame_amt_goal, iter=1):\n",
    "\n",
    "    frame_nums = pf.frame.unique()\n",
    "    frame_diff = abs(frame_amt_goal - len(frame_nums))\n",
    "    operation = frame_amt_goal > len(frame_nums)\n",
    "\n",
    "    values_to_operate = np.array([])\n",
    "\n",
    "    if frame_diff%2 == 1:\n",
    "        central_point = frame_nums[int(len(frame_nums)/2)]\n",
    "        values_to_operate = np.append(values_to_operate, [central_point])\n",
    "        frame_nums = np.delete(frame_nums, [int(len(frame_nums)/2)])\n",
    "        frame_diff -= 1\n",
    "\n",
    "    if frame_diff != 0:\n",
    "        step_val = len(frame_nums)/frame_diff\n",
    "        step_val = 1 if step_val < 1 else step_val\n",
    "        \n",
    "        loop_cnt = len(frame_nums) if frame_diff > len(frame_nums) else frame_diff\n",
    "        values_to_operate = np.append(values_to_operate, frame_nums[[int(i*step_val) for i in range(0, loop_cnt)]])\n",
    "    else:\n",
    "        loop_cnt = 0\n",
    "\n",
    "    if operation:\n",
    "        pf = duplicate_vals(pf, values_to_operate, iter)\n",
    "    else:\n",
    "        pf = remove_vals(pf, values_to_operate)\n",
    "\n",
    "    if frame_diff - loop_cnt != 0:\n",
    "        pf = transform_data(pf, frame_amt_goal, iter+1)\n",
    "\n",
    "    return pf\n",
    "\n",
    "def populate_table(pf, video_data):\n",
    "    frame_num = 0\n",
    "\n",
    "    for frame in pf.frame.unique():\n",
    "        x_vals = list(pf['x'].loc[pf.frame==frame])\n",
    "        y_vals = list(pf['y'].loc[pf.frame==frame])\n",
    "        z_vals = list(pf['z'].loc[pf.frame==frame])\n",
    "\n",
    "        video_data[f'{frame_num}x'] = x_vals\n",
    "        video_data[f'{frame_num}y'] = y_vals\n",
    "        video_data[f'{frame_num}z'] = z_vals\n",
    "        \n",
    "        frame_num += 1\n",
    "\n",
    "    return video_data\n",
    "\n",
    "def create_data_table(pf):\n",
    "    col_labels = ['type','landmark_index']\n",
    "\n",
    "    for i in range(len(pf.frame.unique())):\n",
    "        col_labels.append(f'{i}x')\n",
    "        col_labels.append(f'{i}y')\n",
    "        col_labels.append(f'{i}z')\n",
    "\n",
    "    landmarks = []\n",
    "    types = []\n",
    "\n",
    "    for i in pf.type.unique():\n",
    "        for j in pf.landmark_index.loc[pf.type==i].unique():\n",
    "            landmarks.append(j)\n",
    "            types.append(i)\n",
    "\n",
    "    data = {col: [0.0] * len(types) for col in col_labels}\n",
    "    data['type'] = types\n",
    "    data['landmark_index'] = landmarks\n",
    "\n",
    "    video_data = pd.DataFrame(columns=col_labels, data=data)\n",
    "    video_data = populate_table(pf, video_data)\n",
    "\n",
    "    return video_data\n",
    "\n",
    "def apply_PCA(n_components, x_train, x_test):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(x_train)\n",
    "    return pca.transform(x_train), pca.transform(x_test)\n",
    "\n",
    "def drop_empty_rows(pf):\n",
    "    pf = pf.drop(pf.loc[(pf.x == 0) & (pf.y == 0) & (pf.z == 0)].index, axis=0)\n",
    "    return pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize(tf.Module):\n",
    "  def __init__(self, x):\n",
    "    # Initialize the mean and standard deviation for normalization\n",
    "    self.mean = tf.Variable(tf.math.reduce_mean(x, axis=0))\n",
    "    self.std = tf.Variable(tf.math.reduce_std(x, axis=0)) + 0.001\n",
    "\n",
    "  def norm(self, x):\n",
    "    # Normalize the input\n",
    "    return (x - self.mean)/self.std\n",
    "\n",
    "  def unnorm(self, x):\n",
    "    # Unnormalize the input\n",
    "    return (x * self.std) + self.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [\"alligator/\", \"flower/\", \"kiss/\", \"listen/\", \"orange/\"]\n",
    "aggregated_files = []\n",
    "\n",
    "for folder in folders:\n",
    "    path = \"./asl-kaggle/averaged_by_labels/\"+folder\n",
    "    # path = \"./asl-kaggle/by_labels/\"+folder\n",
    "    parquets = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "\n",
    "    for parquet in parquets[:50]:\n",
    "        parquet_path = path+parquet\n",
    "        pf = pd.read_parquet(parquet_path)\n",
    "        \n",
    "        pf = pf.drop(pf.loc[pf.type==\"face\"].index)\n",
    "        pf = pf.fillna(0)\n",
    "\n",
    "        # pf = default_to_right(pf)\n",
    "        pf = transform_data(pf, 30)\n",
    "        video_data = create_data_table(pf)\n",
    "\n",
    "        # Folder is used as the label here\n",
    "        aggregated_row = np.array([folder])\n",
    "\n",
    "        # Drop unnecessary columns\n",
    "        dropped_columns = video_data.drop(['type', 'landmark_index'], axis=1)\n",
    "\n",
    "        # Calculate mean, median, min, max for each column\n",
    "        basic_stats = dropped_columns.agg(['mean', 'median', 'min', 'max']).values.flatten()\n",
    "\n",
    "        # Calculate skew and kurtosis for each column\n",
    "        skew_kurtosis_stats = dropped_columns.apply(lambda x: pd.Series([skew(x), kurtosis(x)])).values.flatten()\n",
    "        \n",
    "        # Concatenate all the calculated values\n",
    "        aggregated_values = np.concatenate([basic_stats, skew_kurtosis_stats])\n",
    "\n",
    "        # Append the calculated values to aggregated_row\n",
    "        aggregated_row = np.append(aggregated_row, aggregated_values)\n",
    "\n",
    "        aggregated_files.append(aggregated_row)\n",
    "\n",
    "aggregated_pf = pd.DataFrame(aggregated_files)\n",
    "aggregated_pf = aggregated_pf.rename(columns={0: 'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "\n",
    "for i in range(20):\n",
    "    r_state = random.randint(1,1000)\n",
    "    train_dataset = aggregated_pf.sample(frac=0.75, random_state=r_state)\n",
    "    test_dataset = aggregated_pf.drop(train_dataset.index)\n",
    "\n",
    "    x_train, y_train = train_dataset.iloc[:, 1:], train_dataset.iloc[:, 0]\n",
    "    x_test, y_test = test_dataset.iloc[:, 1:], test_dataset.iloc[:, 0]\n",
    "\n",
    "    x_train = tf.convert_to_tensor(x_train, dtype=tf.float32)\n",
    "    x_test = tf.convert_to_tensor(x_test, dtype=tf.float32)\n",
    "\n",
    "    norm_x = Normalize(x_train)\n",
    "    x_train_norm, x_test_norm = norm_x.norm(x_train), norm_x.norm(x_test)\n",
    "\n",
    "    # pca_x_train, pca_x_test = apply_PCA(40, x_train_norm, x_test_norm)\n",
    "    pca_x_train, pca_x_test = x_train_norm, x_test_norm\n",
    "\n",
    "    classif = LogisticRegression(random_state=0, max_iter=10000).fit(pca_x_train, y_train)\n",
    "    # classif = RandomForestClassifier().fit(pca_x_train, y_train)\n",
    "    # classif = SVC(gamma='auto').fit(pca_x_train, y_train)\n",
    "    \n",
    "    y_preds = classif.predict(pca_x_test)\n",
    "    scores.append(accuracy_score(y_test, y_preds))\n",
    "\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zorko\\anaconda3\\envs\\IS460MLApps\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:110: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "found 0 physical cores < 1\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"c:\\Users\\zorko\\anaconda3\\envs\\IS460MLApps\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 217, in _count_physical_cores\n",
      "    raise ValueError(\n",
      "c:\\Users\\zorko\\anaconda3\\envs\\IS460MLApps\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\zorko\\anaconda3\\envs\\IS460MLApps\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\zorko\\anaconda3\\envs\\IS460MLApps\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\zorko\\anaconda3\\envs\\IS460MLApps\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\zorko\\anaconda3\\envs\\IS460MLApps\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\zorko\\anaconda3\\envs\\IS460MLApps\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\zorko\\anaconda3\\envs\\IS460MLApps\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\zorko\\anaconda3\\envs\\IS460MLApps\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\zorko\\anaconda3\\envs\\IS460MLApps\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\zorko\\anaconda3\\envs\\IS460MLApps\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\zorko\\anaconda3\\envs\\IS460MLApps\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\zorko\\anaconda3\\envs\\IS460MLApps\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\zorko\\anaconda3\\envs\\IS460MLApps\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\zorko\\anaconda3\\envs\\IS460MLApps\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\zorko\\anaconda3\\envs\\IS460MLApps\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\zorko\\anaconda3\\envs\\IS460MLApps\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\zorko\\anaconda3\\envs\\IS460MLApps\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\zorko\\anaconda3\\envs\\IS460MLApps\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\zorko\\anaconda3\\envs\\IS460MLApps\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\Users\\zorko\\anaconda3\\envs\\IS460MLApps\\lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    }
   ],
   "source": [
    "# Code source: Gaël Varoquaux\n",
    "#              Andreas Müller\n",
    "# Modified for documentation by Jaques Grobler\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import os\n",
    "\n",
    "names = [\n",
    "    \"Nearest Neighbors\",\n",
    "    \"Linear SVM\",\n",
    "    \"RBF SVM\",\n",
    "    \"Gaussian Process\",\n",
    "    \"Decision Tree\",\n",
    "    \"Random Forest\",\n",
    "    \"Neural Net\",\n",
    "    \"AdaBoost\",\n",
    "    \"Naive Bayes\",\n",
    "    \"QDA\",\n",
    "    \"Logistic Regression\",\n",
    "]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025, random_state=42),\n",
    "    SVC(gamma=2, C=1, random_state=42),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0), random_state=42),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    MLPClassifier(alpha=1, max_iter=10000, random_state=42),\n",
    "    AdaBoostClassifier(algorithm=\"SAMME\", random_state=42),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "    LogisticRegression(max_iter=10000, random_state=42),\n",
    "]\n",
    "\n",
    "os.environ[\"LOKY_MAX_CPU_COUNT\"] = \"12\"\n",
    "\n",
    "scores_per_classif = dict(zip(names, [[] for i in range(len(names))]))\n",
    "\n",
    "for i in range(20):\n",
    "    r_state = random.randint(1,100)\n",
    "    train_dataset = aggregated_pf.sample(frac=0.75, random_state=r_state)\n",
    "    test_dataset = aggregated_pf.drop(train_dataset.index)\n",
    "\n",
    "    x_train, y_train = train_dataset.iloc[:, 1:], train_dataset.iloc[:, 0]\n",
    "    x_test, y_test = test_dataset.iloc[:, 1:], test_dataset.iloc[:, 0]\n",
    "\n",
    "    x_train = tf.convert_to_tensor(x_train, dtype=tf.float32)\n",
    "    x_test = tf.convert_to_tensor(x_test, dtype=tf.float32)\n",
    "\n",
    "    norm_x = Normalize(x_train)\n",
    "    x_train_norm, x_test_norm = norm_x.norm(x_train), norm_x.norm(x_test)\n",
    "\n",
    "    pca_x_train, pca_x_test = apply_PCA(80, x_train_norm, x_test_norm)\n",
    "    # pca_x_train, pca_x_test = x_train_norm, x_test_norm\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        classif = clf\n",
    "        classif.fit(pca_x_train, y_train)\n",
    "        y_pred = classif.predict(pca_x_test)\n",
    "\n",
    "        scores_per_classif[name].append(accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Nearest Neighbors      0.322581\n",
       "Linear SVM             0.458065\n",
       "RBF SVM                0.149194\n",
       "Gaussian Process       0.184677\n",
       "Decision Tree          0.268548\n",
       "Random Forest          0.356452\n",
       "Neural Net             0.467742\n",
       "AdaBoost               0.307258\n",
       "Naive Bayes            0.380645\n",
       "QDA                    0.218548\n",
       "Logistic Regression    0.431452\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(scores_per_classif).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IS460MLApps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
