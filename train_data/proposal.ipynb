{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import mediapipe model for finger and pose detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # drawing utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    \"\"\"\n",
    "    Takes input 'image' and 'model'.\n",
    "    \n",
    "    Applies model to unwriteable image and returns the image with the results.\n",
    "    \"\"\"\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # image is no longer writeable\n",
    "    results = model.process(image)                 # make prediction\n",
    "    image.flags.writeable = True                   # image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    \"\"\"\n",
    "    Applies mask over image.\n",
    "    \"\"\"\n",
    "    # draw face connections\n",
    "    # mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION) # removed as we are only interested in relative position of face\n",
    "    # draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) \n",
    "    # draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) \n",
    "    # draw right hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    \"\"\"\n",
    "    Applies stylised mask over image.\n",
    "    \"\"\"\n",
    "    # # Draw face connections\n",
    "    # mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "    #                          mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "    #                          mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "    #                          ) \n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    \"\"\"\n",
    "    Takes key points from results and converts into an array.\n",
    "    \"\"\"\n",
    "    if results.pose_landmarks:\n",
    "        pose = np.array([[po.x, po.y, po.z] for po in results.pose_landmarks.landmark]).flatten()\n",
    "    else:\n",
    "        pose = np.zeros(33*3)\n",
    "\n",
    "    if results.left_hand_landmarks:\n",
    "        left = np.array([[lh.x, lh.y, lh.z] for lh in results.left_hand_landmarks.landmark]).flatten()\n",
    "    else:\n",
    "        left = np.zeros(21*3)\n",
    "\n",
    "    if results.right_hand_landmarks:\n",
    "        right = np.array([[rh.x, rh.y, rh.z] for rh in results.right_hand_landmarks.landmark]).flatten()\n",
    "    else:\n",
    "        right = np.zeros(21*3)\n",
    "    \n",
    "    return np.concatenate([pose, left, right])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_coordinates(filepath, savepath):\n",
    "    \"\"\"\n",
    "    Takes in the file path of the videos and save path for labels.\n",
    "\n",
    "    Go through each frame of the video, apply detection model and save key points. \n",
    "\n",
    "    Show the masked image for visual confirmation.\n",
    "\n",
    "    Returns a frame number to count total frames processed.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(filepath)\n",
    "    frame_num = 1\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        # while(frame_num < 31):\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            if ret == True:\n",
    "            \n",
    "                image, results = mediapipe_detection(frame, holistic)\n",
    "                draw_styled_landmarks(image, results)\n",
    "                keypoints = extract_keypoints(results)\n",
    "\n",
    "                save_dest = os.path.join(savepath, str(frame_num))\n",
    "\n",
    "                np.save(save_dest, keypoints)\n",
    "\n",
    "                frame_num += 1\n",
    "            \n",
    "                cv2.imshow('frame', image)\n",
    "                if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "                    break\n",
    " \n",
    "            else:\n",
    "                break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    return frame_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Folder creation\n",
    "\n",
    "Create folders for\n",
    "1. Label folders for new actions\n",
    "2. Sub-label folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set array of actions, may be updated when more data samples are added\n",
    "actions = np.array(sorted(['door', 'house', 'again', 'open'])) # sorted to follow folder arrangement\n",
    "\n",
    "# current directory\n",
    "c_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keypoint extraction\n",
    "Here we extract the keypoints of the data (videos) by looping through each set of videos in each action folder, then writing the corresponding keypoints. \n",
    "\n",
    "This will be used later for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently reading video 1: d:\\GitHub\\SSLrecognition\\train_data\\videos\\again\\fq_again_1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wei-z\\anaconda3\\envs\\slproj\\lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently reading video 2: d:\\GitHub\\SSLrecognition\\train_data\\videos\\again\\fq_again_2.mp4\n",
      "Currently reading video 3: d:\\GitHub\\SSLrecognition\\train_data\\videos\\again\\zh_again_1.mp4\n",
      "Currently reading video 4: d:\\GitHub\\SSLrecognition\\train_data\\videos\\again\\zh_again_2.mp4\n",
      "Currently reading video 5: d:\\GitHub\\SSLrecognition\\train_data\\videos\\again\\zh_again_3.mp4\n",
      "Currently reading video 6: d:\\GitHub\\SSLrecognition\\train_data\\videos\\door\\fq_door_1.mp4\n",
      "Currently reading video 7: d:\\GitHub\\SSLrecognition\\train_data\\videos\\door\\k_door_1.mp4\n",
      "Currently reading video 8: d:\\GitHub\\SSLrecognition\\train_data\\videos\\door\\k_door_2.mp4\n",
      "Currently reading video 9: d:\\GitHub\\SSLrecognition\\train_data\\videos\\door\\k_door_3.mp4\n",
      "Currently reading video 10: d:\\GitHub\\SSLrecognition\\train_data\\videos\\door\\zh_door_1.mp4\n",
      "Currently reading video 11: d:\\GitHub\\SSLrecognition\\train_data\\videos\\door\\zh_door_2.mp4\n",
      "Currently reading video 12: d:\\GitHub\\SSLrecognition\\train_data\\videos\\door\\zh_door_3.mp4\n",
      "Currently reading video 13: d:\\GitHub\\SSLrecognition\\train_data\\videos\\house\\fq_house_1.mp4\n",
      "Currently reading video 14: d:\\GitHub\\SSLrecognition\\train_data\\videos\\house\\zh_house_1.mp4\n",
      "Currently reading video 15: d:\\GitHub\\SSLrecognition\\train_data\\videos\\house\\zh_house_2.mp4\n",
      "Currently reading video 16: d:\\GitHub\\SSLrecognition\\train_data\\videos\\house\\zh_house_3.mp4\n",
      "Currently reading video 17: d:\\GitHub\\SSLrecognition\\train_data\\videos\\open\\fq_open_1.mp4\n",
      "Currently reading video 18: d:\\GitHub\\SSLrecognition\\train_data\\videos\\open\\k_open_1.mp4\n",
      "Currently reading video 19: d:\\GitHub\\SSLrecognition\\train_data\\videos\\open\\k_open_2.mp4\n",
      "Currently reading video 20: d:\\GitHub\\SSLrecognition\\train_data\\videos\\open\\k_open_3.mp4\n"
     ]
    }
   ],
   "source": [
    "video_count = 0 # required for processing later\n",
    "for action in actions:\n",
    "    counter = 0 # to count video/extracted file\n",
    "    for video in os.listdir(os.path.join(c_dir, 'videos', action)): # going through each converted video file in the action\n",
    "        counter += 1\n",
    "        video_count += 1\n",
    "        filepath = os.path.join(os.path.join(c_dir, 'videos', action, video))\n",
    "\n",
    "        # NOTE: for each video, save keypoints in new subfolder\n",
    "        # create subfolder if it does not exist\n",
    "        subfolder = str(action) + '_' + str(counter)\n",
    "        if not os.path.isdir(os.path.join(c_dir, 'labels', action, subfolder)):\n",
    "            os.mkdir(os.path.join(c_dir, 'labels', action, subfolder))\n",
    "        # set new savepath\n",
    "        savepath = os.path.join(c_dir, 'labels', action, subfolder)\n",
    "        print(f'Currently reading video {video_count}: {filepath}')\n",
    "        extract_coordinates(filepath, savepath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras_preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'again': 0, 'door': 1, 'house': 2, 'open': 3}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a dictionary for int representation of actions\n",
    "label_map = {label:num for num, label in enumerate(actions)}\n",
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening path: d:\\GitHub\\SSLrecognition\\train_data\\labels\\again\n",
      "Number of instances: 5\n",
      "Number of frames in again_1: 96\n",
      "Number of frames in again_2: 96\n",
      "Number of frames in again_3: 70\n",
      "Number of frames in again_4: 57\n",
      "Number of frames in again_5: 55\n",
      "---------------------------------------------------------------------------\n",
      "Opening path: d:\\GitHub\\SSLrecognition\\train_data\\labels\\door\n",
      "Number of instances: 7\n",
      "Number of frames in door_1: 87\n",
      "Number of frames in door_2: 78\n",
      "Number of frames in door_3: 78\n",
      "Number of frames in door_4: 73\n",
      "Number of frames in door_5: 42\n",
      "Number of frames in door_6: 50\n",
      "Number of frames in door_7: 37\n",
      "---------------------------------------------------------------------------\n",
      "Opening path: d:\\GitHub\\SSLrecognition\\train_data\\labels\\house\n",
      "Number of instances: 4\n",
      "Number of frames in house_1: 105\n",
      "Number of frames in house_2: 57\n",
      "Number of frames in house_3: 59\n",
      "Number of frames in house_4: 67\n",
      "---------------------------------------------------------------------------\n",
      "Opening path: d:\\GitHub\\SSLrecognition\\train_data\\labels\\open\n",
      "Number of instances: 4\n",
      "Number of frames in open_1: 117\n",
      "Number of frames in open_2: 96\n",
      "Number of frames in open_3: 95\n",
      "Number of frames in open_4: 103\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "sequences, labels = [], []  # sequence -> video, labels -> action\n",
    "for action in actions:\n",
    "    no_actions = len(os.listdir(os.path.join(c_dir, 'labels', action)))\n",
    "    print('Opening path:', os.path.join(c_dir, 'labels', action))\n",
    "    print(f'Number of instances: {no_actions}')\n",
    "    for num in range(1, no_actions + 1):\n",
    "        window = []         # window -> single frame\n",
    "        file = str(action) + \"_\" + str(num)\n",
    "        no_frames_per_action = len(os.listdir(os.path.join(c_dir, 'labels', action, file)))\n",
    "        print(f'Number of frames in {file}: {no_frames_per_action}')\n",
    "        for frame_num in range(1, no_frames_per_action + 1):\n",
    "            res = np.load(os.path.join(c_dir, 'labels', action, file,  \"{}.npy\".format(frame_num)))     # res -> coordinate key points\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])\n",
    "    print('-'*75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# due to difference in number of frames, pad x and y\n",
    "x = np.array(pad_sequences(sequences, dtype = 'float', padding = 'post', value = 0))\n",
    "y = pad_sequences(to_categorical(labels).astype(int), dtype = 'int', padding = 'post', value = -1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard, TerminateOnNaN, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for logging of data with TensorBoard\n",
    "log_dir = os.path.join(c_dir, 'Logs')\n",
    "tb_callback = TensorBoard(log_dir = log_dir)\n",
    "\n",
    "# to end training when failure happens ie. loss == nan\n",
    "term = TerminateOnNaN()\n",
    "\n",
    "# to stop training early if there is no change in loss\n",
    "early = EarlyStopping(monitor = 'loss', patience = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences = True, activation = \"relu\", input_shape = (117, 225)))\n",
    "model.add(LSTM(128, return_sequences = True, activation = \"relu\"))\n",
    "model.add(LSTM(64, return_sequences = False, activation = \"relu\"))\n",
    "model.add(Dense(64, activation = \"relu\"))\n",
    "model.add(Dense(32, activation = \"relu\"))\n",
    "model.add(Dense(8, activation = \"relu\"))\n",
    "model.add(Dense(actions.shape[0], activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(optimizer = opt, loss = \"categorical_crossentropy\", metrics = ['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_18 (LSTM)              (None, 117, 64)           74240     \n",
      "                                                                 \n",
      " lstm_19 (LSTM)              (None, 117, 128)          98816     \n",
      "                                                                 \n",
      " lstm_20 (LSTM)              (None, 64)                49408     \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 8)                 264       \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 4)                 36        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 229004 (894.55 KB)\n",
      "Trainable params: 229004 (894.55 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 1.3549 - categorical_accuracy: 0.3333\n",
      "Epoch 2/2000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 1.3549 - categorical_accuracy: 0.3333\n",
      "Epoch 3/2000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 1.3549 - categorical_accuracy: 0.3333\n",
      "Epoch 4/2000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 1.3549 - categorical_accuracy: 0.3333\n",
      "Epoch 5/2000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.3549 - categorical_accuracy: 0.3333\n",
      "Epoch 6/2000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 1.3549 - categorical_accuracy: 0.3333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1760b2ab580>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs = 2000, callbacks = [tb_callback, term, early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 10 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001760B20E4C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 235ms/step\n"
     ]
    }
   ],
   "source": [
    "# take model predictions\n",
    "res = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0]\n",
      " [0 1 0 0]]\n",
      "[[0 1 0 0]\n",
      " [0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print((res == res.max(axis=1, keepdims=True)).astype(int))\n",
    "print(y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "is460proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
