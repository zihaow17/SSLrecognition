{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import mediapipe model for finger and pose detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # drawing utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    \"\"\"\n",
    "    Takes input 'image' and 'model'.\n",
    "    \n",
    "    Applies model to unwriteable image and returns the image with the results.\n",
    "    \"\"\"\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # image is no longer writeable\n",
    "    results = model.process(image)                 # make prediction\n",
    "    image.flags.writeable = True                   # image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    \"\"\"\n",
    "    Applies mask over image.\n",
    "    \"\"\"\n",
    "    # draw face connections\n",
    "    # mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION) # removed as we are only interested in relative position of face\n",
    "    # draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) \n",
    "    # draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) \n",
    "    # draw right hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    \"\"\"\n",
    "    Applies stylised mask over image.\n",
    "    \"\"\"\n",
    "    # # Draw face connections\n",
    "    # mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "    #                          mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "    #                          mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "    #                          ) \n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    \"\"\"\n",
    "    Takes key points from results and converts into an array.\n",
    "    \"\"\"\n",
    "    if results.pose_landmarks:\n",
    "        pose = np.array([[po.x, po.y, po.z] for po in results.pose_landmarks.landmark]).flatten()\n",
    "    else:\n",
    "        pose = np.zeros(33*3)\n",
    "\n",
    "    if results.left_hand_landmarks:\n",
    "        left = np.array([[lh.x, lh.y, lh.z] for lh in results.left_hand_landmarks.landmark]).flatten()\n",
    "    else:\n",
    "        left = np.zeros(21*3)\n",
    "\n",
    "    if results.right_hand_landmarks:\n",
    "        right = np.array([[rh.x, rh.y, rh.z] for rh in results.right_hand_landmarks.landmark]).flatten()\n",
    "    else:\n",
    "        right = np.zeros(21*3)\n",
    "    \n",
    "    return np.concatenate([pose, left, right])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_coordinates(filepath, savepath):\n",
    "    \"\"\"\n",
    "    Takes in the file path of the videos and save path for labels.\n",
    "\n",
    "    Go through each frame of the video, apply detection model and save key points. \n",
    "\n",
    "    Show the masked image for visual confirmation.\n",
    "\n",
    "    Returns a frame number to count total frames processed.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(filepath)\n",
    "    frame_num = 1\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        # while(frame_num < 31):\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            if ret == True:\n",
    "            \n",
    "                image, results = mediapipe_detection(frame, holistic)\n",
    "                draw_styled_landmarks(image, results)\n",
    "                keypoints = extract_keypoints(results)\n",
    "\n",
    "                save_dest = os.path.join(savepath, str(frame_num))\n",
    "\n",
    "                np.save(save_dest, keypoints)\n",
    "\n",
    "                frame_num += 1\n",
    "\n",
    "\n",
    " \n",
    "            else:\n",
    "                break\n",
    "    \n",
    "    cap.release()\n",
    "\n",
    "    return frame_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Folder creation\n",
    "\n",
    "Create folders for\n",
    "1. Label folders for new actions\n",
    "2. Sub-label folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base directory\n",
    "directory_path = '/mnt/d/GitHub/SSLrecognition/train_data/videos'\n",
    "# current directory\n",
    "c_dir = os.getcwd()\n",
    "\n",
    "actions = np.array(sorted([folder for folder in os.listdir(directory_path) if os.path.isdir(os.path.join(directory_path, folder))])) # sorted to follow folder arrangement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keypoint extraction\n",
    "Here we extract the keypoints of the data (videos) by looping through each set of videos in each action folder, then writing the corresponding keypoints. \n",
    "\n",
    "This will be used later for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_count = 0 # required for processing later\n",
    "for action in actions:\n",
    "    counter = 0 # to count video/extracted file\n",
    "    if not os.path.isdir(os.path.join(c_dir, 'labels', action)):\n",
    "        os.mkdir(os.path.join(c_dir, 'labels', action))\n",
    "    for video in os.listdir(os.path.join(c_dir, 'videos', action)): # going through each converted video file in the action\n",
    "        counter += 1\n",
    "        video_count += 1\n",
    "        filepath = os.path.join(os.path.join(c_dir, 'videos', action, video))\n",
    "\n",
    "        # NOTE: for each video, save keypoints in new subfolder\n",
    "        # create subfolder if it does not exist\n",
    "        subfolder = str(action) + '_' + str(counter)\n",
    "        if not os.path.isdir(os.path.join(c_dir, 'labels', action, subfolder)):\n",
    "            os.mkdir(os.path.join(c_dir, 'labels', action, subfolder))\n",
    "        # set new savepath\n",
    "        savepath = os.path.join(c_dir, 'labels', action, subfolder)\n",
    "        print(f'Currently reading video {video_count}: {filepath}')\n",
    "        extract_coordinates(filepath, savepath)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
